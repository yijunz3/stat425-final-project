---
title: "stat425 final project"
author: 
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

```{r}
#Import the original data set
library(readr)
library(lmtest)
library(Metrics)
Fat_Supply_Quantity_Data <- read_csv("Fat_Supply_Quantity_Data.csv")
str(Fat_Supply_Quantity_Data)
```

```{r}
colnames(Fat_Supply_Quantity_Data)
```

## Exploratory Data Analysis

# 2.1 Data cleaning

```{r}
library(dplyr)
library(tidyr)

#Remove meaningless column ,as well as columns with too many 0 values (which means that this kind of food is not supplied in most of countries)
modified_data = Fat_Supply_Quantity_Data[, !(colnames(Fat_Supply_Quantity_Data) %in% c("Country","Unit (all except Population)"))]


#Convert the variable `Undernourished` to be numerical
modified_data$Undernourished[modified_data$Undernourished == "<2.5"] = 2.5
modified_data$Undernourished = as.numeric(modified_data$Undernourished)
#Convert column with more than 90% zero values to be categorical
modified_data$`Sugar Crops` = factor(ifelse(modified_data$`Sugar Crops` == 0,"Not Supplied","Supplied"))
modified_data$`Sugar & Sweeteners` = factor(ifelse(modified_data$`Sugar & Sweeteners` == 0,"Not Supplied","Supplied"))
modified_data$`Alcoholic Beverages` = factor(ifelse(modified_data$`Alcoholic Beverages` == 0,"Not Supplied","Supplied"))
modified_data$`Aquatic Products, Other` = factor(ifelse(modified_data$`Aquatic Products, Other` == 0,"Not Supplied","Supplied"))

#Replace NA values with 0s
modified_data[is.na(modified_data)] = 0
str(modified_data)

```


```{r}
colnames(modified_data)
```


# 2.2 Data Exploration

```{r}
par(mfrow = c(1,2))
#Plot the histogram of the response
hist(modified_data$Deaths,xlab = "Death Rate",main = "Histogram of Death Rates",breaks = 50)

#Plot of the histogram of the response after applying the log transformation
hist(log(modified_data$Deaths),xlab = "Death Rate",main = "Histogram of Death Rates",breaks = 50)

```

```{r}
#Create a graphical display of the correlation matrix of data
library(corrplot)
m = cor(modified_data[,-c(1,4,18,19)])
par(cex=0.65)
corrplot(m,method="circle")
```

# 2.3 Outlier Detection

```{r}

data = Fat_Supply_Quantity_Data
data$Undernourished[data$Undernourished == "<2.5"] = "0"
data$Undernourished = as.numeric(data$Undernourished)

data = data[,!(colnames(data) %in% c("Country", "Unit (all except Population)"))]

data[is.na(data)] = 0

# studentized test
lin = lm(Deaths ~ ., data = modified_data)
jack = rstudent(lin) 
sort(abs(jack), decreasing=TRUE)[1:5]

qt(.05 / (2 * nrow(data)), nrow(data))
```

```{r}
# halfnorm plot with cook's distance

library(faraway)

cook = cooks.distance(lin)
max(cook)

halfnorm(cook, labs=row.names(data), ylab="Cook's distances")
```

# 2.4 Interaction

```{r}

c = colnames(data)
c[c == "Cereals - Excluding Beer"] = "Cereals...Excluding.Beer"
colnames(data) = c

# checking for interactions
int = lm(Deaths ~ Cereals...Excluding.Beer + Undernourished + Cereals...Excluding.Beer*Undernourished,
         data = data)
summary(int)
```

# 2.5 Linearity

```{r}
g1 = lm(Deaths ~ Eggs, data = data)
g2 = lm(Deaths ~ Eggs + I(Eggs^2), data = data)
g3 = lm(Deaths ~ Eggs + I(Eggs^2) + I(Eggs^3), data = data)

summary(g1)
summary(g2)
summary(g3)
```


## Methodology

# 3.1 Model Development
```{r}
#Simple linear regression model
mod_linear <- lm(Deaths~.,data = modified_data)
summary(mod_linear)
```

```{r}
#From the summary table, we are able to observe that there exist colliearity between variables `Confirmed`,`Active`,`Recovered` and the response, we would remove them from the data set.

modified_data = modified_data[,!(colnames(modified_data) %in% c("Confirmed", "Recovered", "Active"))]

str(modified_data)

```

```{r}
# split data into training and testing sets (90 / 10)


modified_data = modified_data[-1 * which(modified_data$Deaths == 0),]

n = nrow(modified_data)
train_indices = sample.int(n, floor(0.9 * n), replace = FALSE)

train_data = modified_data[train_indices,]
test_data = modified_data[-1 * train_indices,]

```


# Variable Selection

```{r}
library(leaps)
b = regsubsets(Deaths~.,data=modified_data)
rs$which
rs=summary(b)
n=dim(modified_data)[1]; msize = 2:9
par(mfrow=c(2,2))
plot(msize,rs$adjr2,xlab="No. of Parameters",ylab="Adjusted Rsquare");
plot(msize,rs$cp,xlab="No. of Parameters",ylab="Mallow's Cp");

Aic=n*log(rs$rss/n) + 2*msize
Bic=n*log(rs$rss/n) + msize*log(n)
plot(msize,Aic,xlab="No. of Parameters",ylab="AIC")
plot(msize,Bic,xlab="No. of Parameters",ylab="BIC")


```
```{r}
rs$which[which.min(Aic),]
```
```{r}
select.var = colnames(rs$which)[rs$which[which.min(Aic),]]
select.var = select.var[-1]
select.var
myfit = lm(Deaths~.,data = modified_data[,c("Animal Products","Animal fats","Miscellaneous","Stimulants","Vegetal Products","Vegetable Oils","Obesity","Deaths")])
summary(myfit)
```
```{r}
# checking for outliers
jack = rstudent(myfit)

n = nrow(modified_data)
qt(.05/(2 * n), 134)

sort(abs(jack), decreasing=TRUE)[1:5] 


```

```{r}
#Do the model diagonistic
bptest(myfit)
par(mfrow=c(2,2))
shapiro.test(resid(myfit))
plot(myfit,which=c(1,2))

```

```{r}
#The p-value is less than 0.05. Thus,we try to apply a variance stabilizing transformation.
myfit_1 <- lm(log(Deaths)~.,data = modified_data[,c("Animal Products","Animal fats","Miscellaneous","Stimulants","Vegetal Products","Vegetable Oils","Obesity","Deaths")])
summary(myfit_1)
```

```{r}
#Do the assumption check again
bptest(myfit_1)
par(mfrow=c(2,2))
shapiro.test(resid(myfit_1))
plot(myfit_1,which=c(1,2))

```

```{r}
#Since log(Deaths) is negative, we can't directly apply box-cox transformation. If anyone have any idea to correct the non-normality, feel free to write your code here.
```



# 3.2 Model Prediction

```{r}
# predicting with our test data
pred_data = predict(myfit, test_data)
pred_data_1 = predict(myfit_1, test_data)
actual_data = test_data$Deaths
actual_data_1 = test_data$Deaths

#Since what we calculate are the log values
RMSE = rmse(actual_data,pred_data)
RMSE
RMSE_1 = rmse(actual_data_1,pred_data_1)
RMSE_1

```

# 3.3 Other Models

```{r}
#We assume all the factor variables are random effect (since they are representatives of populations in some countries), so we would like to build a random effect model.

library(lme4)
mod_random <- lmer(Deaths~1+`Animal Products`+`Animal fats`+`Miscellaneous`+`Cereals - Excluding Beer` + `Eggs`+ `Fish, Seafood`+`Fruits - Excluding Wine`+`Meat`+`Milk - Excluding Butter`+`Offals`+`Oilcrops`+`Pulses`+`Spices`+`Starchy Roots`+`Stimulants`+`Treenuts`+`Vegetal Products`+`Vegetable Oils`+`Vegetables`+`Obesity`+`Undernourished`+Population+(1|`Aquatic Products, Other`)+(1|`Alcoholic Beverages`)+(1|`Sugar Crops`)+(1|`Sugar & Sweeteners`),modified_data,REML=FALSE)

mod_null <- lm(Deaths~1,modified_data)
lrtstat = as.numeric(2*(logLik(mod_random)-logLik(mod_null)))
lrtstat

1 - pchisq(lrtstat,4)
```


```{r}

# predicting with our test data using the mixed effect model
pred_data_random = predict(mod_random, test_data)

actual_data_random = test_data$Deaths

rmse_train = rmse(predict(mod_random, train_data), train_data$Deaths)
RMSE_Random = rmse(actual_data_random,pred_data_random)

print(paste("RMSE Train: ", rmse_train, " RMSE Test: ", rmse_test))
```

# 3.4 Other Model (Optional)

```{r}
# Deep Learning Neural Network

# Note: Due to the limitations in R, this model was created in Python. The Python code can be viewd here: https://jovian.ai/nishantbalepur/stat-425-neural-network
```


# 3.5 Model Comparison

1. Difficulty in interpretation (number of predictors)
```{r}
# We substract 1 due to the intercept
length(coef(myfit))-1
length(coef(myfit_1))-1
length(coef(mod_random))  
```

```{r}
coef(myfit)
coef(myfit_1)
coef(mod_random)
```

2. RMSE
```{r}
library(ggplot2)

rmse_lin_train = rmse(predict(myfit, train_data), train_data$Deaths)
rmse_lin_test = rmse(predict(myfit, test_data), test_data$Deaths)

rmse_tran_train = rmse(predict(myfit_1, train_data), train_data$Deaths)
rmse_tran_test = rmse(predict(myfit_1, test_data), test_data$Deaths)

rmse_rand_train = rmse(predict(mod_random, train_data), train_data$Deaths)
rmse_rand_test = rmse(predict(mod_random, test_data), test_data$Deaths)

rmse_nn_train = 0.19434809684753418
rmse_nn_test = 0.147894486784935

rmse_data = data.frame(y = c(rmse_lin_train, rmse_lin_test, rmse_tran_train, rmse_tran_test, rmse_rand_train, rmse_rand_test, rmse_nn_train, rmse_nn_test), x = c("Linear", "Linear", "Linear Transformed", "Linear Transformed", "Mixed Effects", "Mixed Effects", "Neural Network", "Neural Network"), dataset = rep(c("Train", "Test"), times = 4))

rmse_data1 = data.frame(y = c(rmse_lin_train, rmse_lin_test, rmse_rand_train, rmse_rand_test, rmse_nn_train, rmse_nn_test), x = c("Linear", "Linear", "Mixed Effects", "Mixed Effects", "Neural Network", "Neural Network"), dataset = rep(c("Train", "Test"), times = 3))

ggplot(rmse_data, aes(fill = dataset, y = y, x = x)) + geom_bar(position="dodge", stat="identity") + xlab("Model") + ylab("RMSE")
ggplot(rmse_data1, aes(fill = dataset, y = y, x = x)) + geom_bar(position="dodge", stat="identity")+ xlab("Model") + ylab("RMSE")

View(rmse_data)
```

