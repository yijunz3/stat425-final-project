---
title: "stat425 final project"
author: 
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

```{r}
#Import the original data set
library(readr)
library(lmtest)
library(Metrics)
Fat_Supply_Quantity_Data <- read_csv("Fat_Supply_Quantity_Data.csv")
str(Fat_Supply_Quantity_Data)
```

```{r}
colnames(Fat_Supply_Quantity_Data)
```

## Exploratory Data Analysis

# 2.1 Data cleaning

```{r}
library(dplyr)
library(tidyr)

#Remove meaningless column ,as well as columns with too many 0 values (which means that this kind of food is not supplied in most of countries)
modified_data = Fat_Supply_Quantity_Data[, !(colnames(Fat_Supply_Quantity_Data) %in% c("Country","Unit (all except Population)"))]


#Convert the variable `Undernourished` to be numerical
modified_data$Undernourished[modified_data$Undernourished == "<2.5"] = 2.5
modified_data$Undernourished = as.numeric(modified_data$Undernourished)
#Convert column with more than 90% zero values to be categorical
modified_data$`Sugar Crops` = factor(ifelse(modified_data$`Sugar Crops` == 0,"Not Supplied","Supplied"))
modified_data$`Sugar & Sweeteners` = factor(ifelse(modified_data$`Sugar & Sweeteners` == 0,"Not Supplied","Supplied"))
modified_data$`Alcoholic Beverages` = factor(ifelse(modified_data$`Alcoholic Beverages` == 0,"Not Supplied","Supplied"))
modified_data$`Aquatic Products, Other` = factor(ifelse(modified_data$`Aquatic Products, Other` == 0,"Not Supplied","Supplied"))

#Replace NA values with 0s
modified_data[is.na(modified_data)] = 0
str(modified_data)

```


```{r}
colnames(modified_data)
```


# 2.2 Data Exploration

```{r}
par(mfrow = c(1,2))
#Plot the histogram of the response
hist(modified_data$Deaths,xlab = "Death Rate",main = "Histogram of Death Rates",breaks = 50)

#Plot of the histogram of the response after applying the log transformation
hist(log(modified_data$Deaths),xlab = "Death Rate",main = "Histogram of Death Rates",breaks = 50)

```

```{r}
#Create a graphical display of the correlation matrix of data
library(corrplot)
m = cor(modified_data[,-c(1,4,18,19)])
par(cex=0.65)
corrplot(m,method="circle")
```

## Methodology

# 3.1 Model Development
```{r}
#Simple linear regression model
mod_linear <- lm(Deaths~.,data = modified_data)
summary(mod_linear)
```

```{r}
#From the summary table, we are able to observe that there exist colliearity between variables `Confirmed`,`Active`,`Recovered` and the response, we would remove them from the data set.

modified_data = modified_data[,!(colnames(modified_data) %in% c("Confirmed", "Recovered", "Active"))]

str(modified_data)

```

```{r}
# split data into training and testing sets (90 / 10)


modified_data = modified_data[-1 * which(modified_data$Deaths == 0),]

n = nrow(modified_data)
train_indices = sample.int(n, floor(0.9 * n), replace = FALSE)

train_data = modified_data[train_indices,]
test_data = modified_data[-1 * train_indices,]

```


# Variable Selection

```{r}
library(leaps)
b = regsubsets(Deaths~.,data=modified_data)
rs$which
rs=summary(b)
n=dim(modified_data)[1]; msize = 2:9
par(mfrow=c(2,2))
plot(msize,rs$adjr2,xlab="No. of Parameters",ylab="Adjusted Rsquare");
plot(msize,rs$cp,xlab="No. of Parameters",ylab="Mallow's Cp");

Aic=n*log(rs$rss/n) + 2*msize
Bic=n*log(rs$rss/n) + msize*log(n)
plot(msize,Aic,xlab="No. of Parameters",ylab="AIC")
plot(msize,Bic,xlab="No. of Parameters",ylab="BIC")


```
```{r}
rs$which[which.min(Aic),]
```
```{r}
select.var = colnames(rs$which)[rs$which[which.min(Aic),]]
select.var = select.var[-1]
select.var
myfit = lm(Deaths~.,data = modified_data[,c("Animal Products","Animal fats","Miscellaneous","Stimulants","Vegetal Products","Vegetable Oils","Obesity","Deaths")])
summary(myfit)
```
```{r}
# checking for outliers
jack = rstudent(myfit)

n = nrow(modified_data)
qt(.05/(2 * n), 134)

sort(abs(jack), decreasing=TRUE)[1:5] 


```

```{r}
#Do the model diagonistic
bptest(myfit)
par(mfrow=c(2,2))
shapiro.test(resid(myfit))
plot(myfit,which=c(1,2))

```

```{r}
#The p-value is less than 0.05. Thus,we try to apply a variance stabilizing transformation.
myfit_1 <- lm(log(Deaths)~.,data = modified_data[,c("Animal Products","Animal fats","Miscellaneous","Stimulants","Vegetal Products","Vegetable Oils","Obesity","Deaths")])
summary(myfit_1)
```

```{r}
#Do the assumption check again
bptest(myfit_1)
par(mfrow=c(2,2))
shapiro.test(resid(myfit_1))
plot(myfit_1,which=c(1,2))

```

```{r}

#Since log(Deaths) is negative, we can't directly apply box-cox transformation. If anyone have any idea to correct the non-normality, feel free to write your code here.

```



# 3.2 Model Prediction

```{r}
# predicting with our test data
pred_data = predict(myfit, test_data)
pred_data_1 = predict(myfit_1, test_data)
actual_data = test_data$Deaths
actual_data_1 = test_data$Deaths

#Since what we calculate are the log values
RMSE = rmse(actual_data,pred_data)
RMSE
RMSE_1 = rmse(actual_data_1,pred_data_1)
RMSE_1

```

# 3.3 Other Models

```{r}
#We assume all the factor variables are random effect (since they are representatives of populations in some countries), so we would like to build a random effect model.

library(lme4)
mod_random <- lmer(Deaths~1+`Animal Products`+`Animal fats`+`Miscellaneous`+`Cereals - Excluding Beer` + `Eggs`+ `Fish, Seafood`+`Fruits - Excluding Wine`+`Meat`+`Milk - Excluding Butter`+`Offals`+`Oilcrops`+`Pulses`+`Spices`+`Starchy Roots`+`Stimulants`+`Treenuts`+`Vegetal Products`+`Vegetable Oils`+`Vegetables`+`Obesity`+`Undernourished`+Population+(1|`Aquatic Products, Other`)+(1|`Alcoholic Beverages`)+(1|`Sugar Crops`)+(1|`Sugar & Sweeteners`),modified_data,REML=FALSE)

mod_null <- lm(Deaths~1,modified_data)
lrtstat = as.numeric(2*(logLik(mod_random)-logLik(mod_null)))
lrtstat

1 - pchisq(lrtstat,4)

```

```{r}

# predicting with our test data using the mixed effect model
pred_data_random = predict(mod_random, test_data)

actual_data_random = test_data$Deaths

rmse(actual_data_random,pred_data_random)
```

# 3.4 Other Model (Optional)

```{r}
#If you want to create other model (polynomial, lasso, etc.), please add your code here.
```


# 3.5 Model Comparison

1. Difficulty in interpretation (number of predictors)
```{r}
#We substract 1 due to the intercept
length(coef(myfit))-1
length(coef(myfit_1))-1
length(coef(mod_random))  
```

```{r}
coef(myfit)
coef(myfit_1)
coef(mod_random)
```

2. RMSE
```{r}
data.frame(
  "RMSE of multiple linear model" = RMSE,
  "RMSE of multiple linear model after transformation" = RMSE_1,
  "RMSE of random effect model" = RMSE_random
)

```


